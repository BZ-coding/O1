# LLM论文学习
# DeepSeek-V3
## FIM策略（Fill-in-Middle）
- 目标：通过补充上下文中间内容，提升逻辑推理能力
- 实现：不同于传统单向预测，强制模型学习中间填空
- 优势：增强长距离依赖捕捉能力，更符合真实推理场景
## 混合精度训练
- 潜在作用：可能通过数值精度变化增强模型鲁棒性
- 机制猜想：低精度计算引入噪声正则化，增强泛化能力
## 训练策略
- 预训练阶段：分阶段恒定学习率（除了warmup）
- 后训练阶段：150w条sft样本。2epoch；采用5e-6~1e-6低学习率微调
- 数学/代码领域：使用R1格式长思考数据增强
### GRPO策略（Group Relative Policy Optimization）
- 核心思想：组内样本相对评估代替绝对得分
- 评估方式：支持规则打分器（如格式验证）或奖励模型
- 创新融合：结合宪法AI方法，用模型自身投票作为反馈源
    - 这种方法产生了显着的对齐效果，显着提高了 DeepSeek-V3 在主观评价中的性能。通过整合额外的宪法输入，DeepSeek-V3 可以优化宪法方向。
其实相当于是一组数据内的相对打分。
打分器用规则还是rm都行。
## 来自R1模型的蒸馏数据
### 训练一个单独的数据生成模型
用模型原始回答和r1回答，训练一个可以融合生成r1格式回答的数据生成模型。
使用拒绝采样控制样本质量。
## 附加模块(MTP)
可以附加模块同时进行训练。
DeepSeek是新增了MTP的loss作为附加训练目标。
我们可以尝试新增浮点输出头，输出token级别的关键token权重，可以作为loss权重一起训练。




---------------------------------------------------------------------------------------------------------------------------------




# DeepSeek-R1
通过纯RL方法自我演化。
我们也可以使用相同的思路，至于GRPO应该不是关键，我们可以使用裁判模型。演化出具备长思维和最后总结的格式，且偏向于裁判模型的L1模型。
## 训练
1. 使用数千条冷启动数据（CoT）来微调 DeepSeek-V3-Base 模型，而不是sft模型。
    1. 以长 CoT 的小样本提示为例，直接提示模型生成带有反射和验证的详细答案，以可读的格式收集 DeepSeek-R1-Zero 输出，并通过人工注释器进行后处理来细化结果。
2. 进行RL，使用GRPO。
    1. 直接RL，不用冷启动，会有可读性差和语言混合等问题。
3. 通过对 RL 检查点进行拒绝采样来创建新的 SFT 数据，并结合写作、事实 QA 和自我认知等领域 DeepSeek-V3 的监督数据，然后重新训练 DeepSeek-V3-Base 模型。
    1. 600k个推理相关的样本。
    2. 200k 个与推理无关的训练样本。
    3. 2 epoch
4. 辅助强化学习阶段
    1. 为了进一步使模型与人类偏好保持一致，旨在提高模型的有用性和无害性，同时细化其推理能力。
    2. 使用奖励信号和多样化的提示分布的组合来训练模型。
        1. 对于推理数据，我们遵循 DeepSeek-R1-Zero 中概述的方法，该方法利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程。
        2. 对于一般数据，我们求助于奖励模型来捕获复杂和细微的场景中的人类偏好。
## 关键突破
- 无SFT的RL训练：首次验证纯RL可激发LLM推理能力
- 数据合成策略：通过模型自生成+人工修正构建高质量数据
- 强化学习：
    - 推理领域：基于规则奖励（数学/代码/逻辑）
    - 通用领域：基于奖励模型捕捉复杂偏好
## 重要发现
- 提示敏感性：零样本直接描述效果优于少样本提示
- 格式重要性：强制结构化输出可显著提升推理质量
- 自我进化机制：
    - 自我验证：在推理过程中自动检查错误
    - 反思能力：发现错误后能调整策略重新尝试
## 启示
- 模型蒸馏：将DeepSeek-R1等推理模型蒸馏到qwen等模型，会比直接对这些模型训RL要更好。
    - 使用以上800k个样本。
- 模块化扩展：尝试添加附加模块，以增强关键信息捕捉
- 直接将 RL 应用于基础模型，这种方法允许模型探索思维链 (CoT) 来解决复杂问题，从而产生 DeepSeek-R1-Zero 。
    - DeepSeekR1-Zero 展示了自我验证、反射和生成长 CoTs 等功能，标志着研究界的一个重要里程碑。值得注意的是，这是第一个验证llm推理能力可以通过RL纯粹激励的开放研究，而不需要SFT。这一突破为该领域未来进步铺平了道路。
- 其实所谓的Aha时刻，在agent时就有所体现：agent可以发现之前的调用结果不对，从而调整搜索语句等输入，重新调用agent。
    - 所以可以理解为是在长上下文中自然出现的一种注意力推理现象。
    - 但显式的提出Aha时刻，的确更有指导方向。
- deepseek说迭代训练是更好的方法。我们需要找到可以启动数据飞轮的方法，用合成数据自己迭代？
    while True:
    从种子生成数据 → 规则+裁判模型拒绝采样 → 模型训练 → 评估筛选模型 → 生成种子数据
- 对提示很敏感。
    - 少样本提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零样本设置指定输出格式以获得最佳结果。
