# LLM论文学习
# DeepSeek-V3
## FIM策略（Fill-in-Middle）
- 目标：通过补充上下文中间内容，提升逻辑推理能力
- 实现：不同于传统单向预测，强制模型学习中间填空
- 优势：增强长距离依赖捕捉能力，更符合真实推理场景
## 混合精度训练
- 潜在作用：可能通过数值精度变化增强模型鲁棒性
- 机制猜想：低精度计算引入噪声正则化，增强泛化能力
## 训练策略
- 预训练阶段：分阶段恒定学习率（除了warmup）
- 后训练阶段：150w条sft样本。2epoch；采用5e-6~1e-6低学习率微调
- 数学/代码领域：使用R1格式长思考数据增强
### GRPO策略（Group Relative Policy Optimization）
- 核心思想：组内样本相对评估代替绝对得分
- 评估方式：支持规则打分器（如格式验证）或奖励模型
- 创新融合：结合宪法AI方法，用模型自身投票作为反馈源
    - 这种方法产生了显着的对齐效果，显着提高了 DeepSeek-V3 在主观评价中的性能。通过整合额外的宪法输入，DeepSeek-V3 可以优化宪法方向。
其实相当于是一组数据内的相对打分。
打分器用规则还是rm都行。
## 来自R1模型的蒸馏数据
### 训练一个单独的数据生成模型
用模型原始回答和r1回答，训练一个可以融合生成r1格式回答的数据生成模型。
使用拒绝采样控制样本质量。
## 附加模块(MTP)
可以附加模块同时进行训练。
DeepSeek是新增了MTP的loss作为附加训练目标。
我们可以尝试新增浮点输出头，输出token级别的关键token权重，可以作为loss权重一起训练。




---




# DeepSeek-R1
通过纯RL方法自我演化。
我们也可以使用相同的思路，至于GRPO应该不是关键，我们可以使用裁判模型。演化出具备长思维和最后总结的格式，且偏向于裁判模型的L1模型。
## 训练
1. 使用数千条冷启动数据（CoT）来微调 DeepSeek-V3-Base 模型，而不是sft模型。
    1. 以长 CoT 的小样本提示为例，直接提示模型生成带有反射和验证的详细答案，以可读的格式收集 DeepSeek-R1-Zero 输出，并通过人工注释器进行后处理来细化结果。
2. 进行RL，使用GRPO。
    1. 直接RL，不用冷启动，会有可读性差和语言混合等问题。
3. 通过对 RL 检查点进行拒绝采样来创建新的 SFT 数据，并结合写作、事实 QA 和自我认知等领域 DeepSeek-V3 的监督数据，然后重新训练 DeepSeek-V3-Base 模型。
    1. 600k个推理相关的样本。
    2. 200k 个与推理无关的训练样本。
    3. 2 epoch
4. 辅助强化学习阶段
    1. 为了进一步使模型与人类偏好保持一致，旨在提高模型的有用性和无害性，同时细化其推理能力。
    2. 使用奖励信号和多样化的提示分布的组合来训练模型。
        1. 对于推理数据，我们遵循 DeepSeek-R1-Zero 中概述的方法，该方法利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程。
        2. 对于一般数据，我们求助于奖励模型来捕获复杂和细微的场景中的人类偏好。
## 关键突破
- 无SFT的RL训练：首次验证纯RL可激发LLM推理能力
- 数据合成策略：通过模型自生成+人工修正构建高质量数据
- 强化学习：
    - 推理领域：基于规则奖励（数学/代码/逻辑）
    - 通用领域：基于奖励模型捕捉复杂偏好
## 重要发现
- 提示敏感性：零样本直接描述效果优于少样本提示
- 格式重要性：强制结构化输出可显著提升推理质量
- 自我进化机制：
    - 自我验证：在推理过程中自动检查错误
    - 反思能力：发现错误后能调整策略重新尝试
## 启示
- 模型蒸馏：将DeepSeek-R1等推理模型蒸馏到qwen等模型，会比直接对这些模型训RL要更好。
    - 使用以上800k个样本。
- 模块化扩展：尝试添加附加模块，以增强关键信息捕捉
- 直接将 RL 应用于基础模型，这种方法允许模型探索思维链 (CoT) 来解决复杂问题，从而产生 DeepSeek-R1-Zero 。
    - DeepSeekR1-Zero 展示了自我验证、反射和生成长 CoTs 等功能，标志着研究界的一个重要里程碑。值得注意的是，这是第一个验证llm推理能力可以通过RL纯粹激励的开放研究，而不需要SFT。这一突破为该领域未来进步铺平了道路。
- 其实所谓的Aha时刻，在agent时就有所体现：agent可以发现之前的调用结果不对，从而调整搜索语句等输入，重新调用agent。
    - 所以可以理解为是在长上下文中自然出现的一种注意力推理现象。
    - 但显式的提出Aha时刻，的确更有指导方向。
- deepseek说迭代训练是更好的方法。我们需要找到可以启动数据飞轮的方法，用合成数据自己迭代？
    - 走合成数据道路，用深度思考模型去生成数据(配合裁判模型)，训练普通模型
        > while True:
        > 从种子生成数据 → 规则+裁判模型拒绝采样 → 模型训练 → 评估筛选模型 → 生成种子数据
- 对提示很敏感。
    - 少样本提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零样本设置指定输出格式以获得最佳结果。




---




# KIMI-K1.5
也是使用RL，而不是用MCTS、PRM等。
也有long2short等。
## RL Prompt
RL 提示集的质量和多样性在确保强化学习的有效性方面起着至关重要的作用。
- 多样化的覆盖范围：提示应该跨越广泛的学科，例如 STEM、编码和一般推理，以增强模型的适应性并确保跨不同领域的广泛适用性。
- 平衡难度：提示集应包括分布良好的简单、中等和困难的问题范围，以促进渐进式学习并防止对特定复杂性级别的过度拟合。
- 准确的可评估性：提示应该允许验证者的客观和可靠的评估，确保模型性能是根据正确的推理而不是表面模式或随机猜测来衡量的。
利用模型自己的能力自适应地评估每个提示的难度。具体来说，对于每个提示，SFT 模型使用相对较高的采样温度生成十次的答案。然后计算通率并将其用作提示难度的代理——通率越低，难度越高。
### Reward Hacking
一些复杂的推理问题可能具有相对简单且易于猜测的答案，导致误报验证——其中模型通过不正确的推理过程到达正确答案。为了解决这个问题，我们排除了容易出现此类错误的问题，例如多项选择、真/假和基于证明的问题。此外，对于一般的问答任务，我们提出了一种简单而有效的方法来识别和删除易于黑客的提示。具体来说，我们提示模型在没有任何 CoT 推理步骤的情况下猜测潜在答案。如果模型预测 N 次尝试中的正确答案，则认为提示太容易黑客攻击并删除。
## 长Cot监督数据
利用RL prompt，构建长cot数据。
## 训练思路
### 规划算法
我们将规划算法视为直接作用于一系列推理步骤 A(·|z1, z2,.）。在这个框架中，规划算法使用的搜索树中存储的所有信息都被展平为提供给算法的完整上下文。这为生成高质量的 CoT 提供了一个有趣的视角：我们可以潜在地训练一个模型来近似这个过程，而不是显式地构建搜索树并实现规划算法。在这里，思想的数量（即语言标记）类似于传统上分配给规划算法的计算预算。长上下文窗口的最新进展有助于训练和测试阶段之间的无缝可扩展性。
### 训练思路
通过扩大 RL 训练，我们的目标是训练一个模型，该模型利用了简单的基于提示的 CoT 和规划增强 CoT 的优势。该模型在推理过程中仍然自回归采样语言序列，从而避免了部署过程中高级规划算法所需的复杂并行化的需要。然而，与简单的基于提示的方法的一个关键区别是模型不应该仅仅遵循一系列推理步骤。相反，它还应该通过利用整套探索的想法作为上下文信息来学习关键的规划技能，包括错误识别、回溯和解决方案细化。
